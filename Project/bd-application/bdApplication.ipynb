{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing - replace umlauts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umlauts(word):\n",
    "    \"\"\"\n",
    "    Replace umlauts for a given text\n",
    "    \n",
    "    :param word: text as string\n",
    "    :return: manipulated text as str\n",
    "    \"\"\"\n",
    "    \n",
    "    tempVar = word  \n",
    "    \n",
    "    tempVar = tempVar.replace('ä', 'ae')\n",
    "    tempVar = tempVar.replace('ö', 'oe')\n",
    "    tempVar = tempVar.replace('ü', 'ue')\n",
    "    tempVar = tempVar.replace('Ä', 'Ae')\n",
    "    tempVar = tempVar.replace('Ö', 'Oe')\n",
    "    tempVar = tempVar.replace('Ü', 'Ue')\n",
    "    tempVar = tempVar.replace('ß', 'ss')\n",
    "    \n",
    "    return tempVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "text = umlauts(text)\n",
    "text = ' '.join((text.strip('\\n').split()))\n",
    "\n",
    "text_file = open(\"test.txt\", \"w\")\n",
    "n = text_file.write(text)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hier beginnt die eigentliche BD-Anwendung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we would use kubernetes then run this instead of next cell\n",
    "'''conf=SparkConf()\\\n",
    "        .setMaster(local[*])\\\n",
    "        .setappName(\"WordCount\")\\\n",
    "        .setExecutorEnv(\"spark.executor.memory\",\"4g\")\\\n",
    "        .setExecutorEnv(\"spark.driver.memory\",\"4g\")\n",
    "  spark=SparkSession.builder\\\n",
    "        .config(conf=conf)\\\n",
    "        .getOrCreate()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"WordCount\")\\\n",
    "    .getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hier eine txt der Datenbank einhängen für den wordcount\n",
    "file = 'test.txt'\n",
    "df = sc.textFile(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude punctuation\n",
    "def lower_clean_str(x):\n",
    "    punc='!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~-'\n",
    "    lowercased_str = x.lower()\n",
    "    for ch in punc:\n",
    "        lowercased_str = lowercased_str.replace(ch, '')\n",
    "    return lowercased_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punktuation and transform to lowercase\n",
    "df = df.map(lower_clean_str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split sentences into list of words\n",
    "df = df.flatMap(lambda satir: satir.split(\" \")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude whitespaces\n",
    "df = df.filter(lambda x:x!='') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many times each word occurs\n",
    "count = df.map(lambda word:(word,1))\n",
    "countRBK = count.reduceByKey(lambda x,y:(x+y)).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank words\n",
    "countRBK = countRBK.map(lambda x:(x[1],x[0]))\n",
    "countRBK = countRBK.sortByKey(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude stopwords\n",
    "## if normal importing doesn't work:   ##\n",
    "## python -m nltk.downloader stopwords ##\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords =stopwords.words('german')\n",
    "\n",
    "german_stopwords = []\n",
    "for word in stopwords:\n",
    "    german_stopwords.append(umlauts(word))\n",
    "\n",
    "countRBK = countRBK.filter(lambda x: x[1] not in german_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 'internet'),\n",
       " (8, 'sagte'),\n",
       " (6, 'bett'),\n",
       " (6, 'frau'),\n",
       " (6, 'gesicht'),\n",
       " (5, 'beim'),\n",
       " (5, 'fenster'),\n",
       " (5, 'ganz'),\n",
       " (5, 'hand'),\n",
       " (5, 'hoch')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countRBK.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
